{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# According to wiki \"sumary of lstm_speech_siamese\", we can say the one shot may not be the best net structure for the problem. \n",
    "\n",
    "Here I try to build another net structure. \n",
    "\n",
    "input:\n",
    "choose an anchor word, and random other words. combine all mfcc into input. the length of inputs may change. \n",
    "Output:\n",
    "the one hot vector, the length will be length(input) - 1. [0, 0, 0, 1, .....]. the 1 means that words is the best match for anchor. \n",
    "the output can be fixed to 128. that means the input cannot beyond 128. in another word, the max commands is not beyond 128. \n",
    "\n",
    "the network\n",
    "\n",
    "### mfcc->TimeDistributed(Dense)->gru--|\n",
    "### mfcc->TimeDistributed(Dense)->gru--|\n",
    "### ....................      --|-lstm(dynamic input->1024 vector)->dense(1024)->dense(256)->dense(128, sigmoid)->output\n",
    "### mfcc->TimeDistributed(Dense)->gru--|\n",
    "### mfcc->TimeDistributed(Dense)->gru--|\n",
    "\n",
    "the point is how to build a dynamic lstm input. \n",
    "\n",
    "Keep mfcc pre-process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import hashlib\n",
    "import math, time, datetime\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "import shutil\n",
    "#print(sys.executable)\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import graphviz\n",
    "import numpy as np\n",
    "import librosa as rosa\n",
    "import librosa.display\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import numpy as np\n",
    "from sklearn import preprocessing #copy from echo 1110/2018\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Lambda, BatchNormalization, Activation, LSTM, GRU, concatenate, TimeDistributed\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from scipy.spatial import distance\n",
    "from tensorflow.keras.utils import multi_gpu_model\n",
    "#from tensorflow.python import debug as tf_debug\n",
    "#from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
    "#from tensorflow.python.ops import io_ops\n",
    "#from tensorflow.python.platform import gfile\n",
    "#from tensorflow.python.util import compat\n",
    "import local_config\n",
    "default_improving_torlerence = 32 # for how many time no acc improving then terminate the training.\n",
    "default_number_of_mfcc=128\n",
    "default_sample_rate=16000\n",
    "default_hop_length=512 \n",
    "default_wav_duration=1 # 1 second\n",
    "default_train_samples=10000\n",
    "default_test_samples=100\n",
    "default_epochs=10\n",
    "default_batch_size=1024\n",
    "default_sequence_size=5 # the default sequence size for how many wavs compare to anchor. \n",
    "default_train_wanted_words=[\"one\", \"two\",  \"backward\", \"bird\", \"cat\", \"dog\", \"five\", \"forward\", \"four\", \"go\", \"happy\", \"house\", \"learn\", \"left\", \"marvin\", \"nine\", \"no\", \"off\", \"right\", \"seven\", \"sheila\", \"stop\", \"three\", \"tree\",  \"wow\", \"zero\",\"up\"]\n",
    "default_test_wanted_words=[\"bed\", \"eight\", \"visual\", \"follow\"]\n",
    "\n",
    "\n",
    "\n",
    "default_kernel_regularizer=regularizers.l2(0.01)\n",
    "default_activity_regularizer=None #regularizers.l1(0.01)\n",
    "\n",
    "#sess = K.get_session()\n",
    "#sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "#K.set_session(sess)\n",
    "\n",
    "#set backgroud noise as the base mfcc input\n",
    "default_noise_background_wav=local_config.speech_data_dir+\"/_background_noise_/dude_miaowing.wav\"\n",
    "base_loader, base_sample_rate = rosa.load(default_noise_background_wav, sr=default_sample_rate, duration=1.0)\n",
    "base_mfcc = rosa.feature.mfcc(y=base_loader, sr=base_sample_rate, n_mfcc=default_number_of_mfcc)\n",
    "#print(base_mfcc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One shot keyword trigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here is another implementation of one-shot learning of keyword trigger with librosa mfcc. \n",
    "librosa cannot put into tensorflow graph. so mfcc computation will be done before conv network. \n",
    "that means load_wav_mfcc has to convert all wav file to mfcc vector. \n",
    "Here i have to understand\n",
    "    1, what is the good mfcc vector dimension. 20, 127 may not be the right input for conv network. \n",
    "    2, even the mfcc output of librosa is not the same as tensorflow contrib.decode wav, it is enough if it has all audio feature. put librosa mfcc output as input of conv net, it will do good learning about feature abstraction. \n",
    "    3, conv net may not be that difficult. just like conv2d -> maxpooling -> conv2d->flatten->dense with softmax. \n",
    "    4, build the train network with librosa and conv net.\n",
    "    5, take the dense vector output as feature extractor. \n",
    "    6, build siamese network with the feature extractor. \n",
    "    7, may add couples of dense layer to learn the feature mapping and comparation of siamese. \n",
    "    8, if that works, we get an one-shot learning for key word trigger...\n",
    "    9, in reality, we still have to work out how to split the audio stream into audio clip as the input the librosa mfcc.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract MFCC from wav file\n",
    "what is the wav parameter for MFCC output\n",
    "\n",
    "tensorflow speech command parameter \n",
    "{'desired_samples': 16000, 'window_size_samples': 480, 'window_stride_samples': 160, 'spectrogram_length': 98, 'fingerprint_width': 40, 'fingerprint_size': 3920, 'label_count': 12, 'sample_rate': 16000, 'preprocess': 'mfcc', 'average_window_width': -1}\n",
    "\n",
    "Mel-frequency cepstral coefficients (MFCCs)\n",
    "Parameters:\t\n",
    "y:np.ndarray [shape=(n,)] or None\n",
    "audio time series\n",
    "sr:number > 0 [scalar]\n",
    "sampling rate of y\n",
    "S:np.ndarray [shape=(d, t)] or None\n",
    "log-power Mel spectrogram\n",
    "n_mfcc: int > 0 [scalar]\n",
    "number of MFCCs to return\n",
    "Returns:\t\n",
    "M:np.ndarray [shape=(n_mfcc, t)]\n",
    "MFCC sequence\n",
    "\n",
    "need more study about MFCC output\n",
    "\n",
    "\n",
    "## How to calculate the lenght of mfcc vector\n",
    "Short Answer\n",
    "\n",
    "You can specify the change the length by changing the parameters used in the stft calculations. The following code will double the size of your output (20 x 113658)\n",
    "\n",
    "data = librosa.feature.mfcc(y=y, sr=sr, n_fft=1012, hop_length=256, n_mfcc=20)\n",
    "Long Answer\n",
    "\n",
    "Librosa's librosa.feature.mfcc() function really just acts as a wrapper to librosa's librosa.feature.melspectrogram() function (which is a wrapper to librosa.core.stft and librosa.filters.mel functions).\n",
    "\n",
    "All of the parameters pertaining to segementation of the audio signal - namely the frame and overlap values - are specified utilized in the Mel-scaled power spectrogram function (with other tune-able parameters specified for nested core functions). You specify these parameters as keyword arguments in the librosa.feature.mfcc() function.\n",
    "\n",
    "All extra **kwargs parameters are fed to librosa.feature.melspectrogram() and subsequently to librosa.filters.mel()\n",
    "\n",
    "By Default, the Mel-scaled power spectrogram window and hop length are the following:\n",
    "\n",
    "n_fft=2048\n",
    "\n",
    "hop_length=512\n",
    "\n",
    "So assuming you used the default sample rate (sr=22050), the output of your mfcc function makes sense:\n",
    "\n",
    "output length = (seconds) * (sample rate) / (hop_length)\n",
    "\n",
    "(1319) * (22050) / (512) = 56804 samples\n",
    "\n",
    "\n",
    "the mfcc vector size is 128 * 32   \n",
    "\n",
    "1 * 16000/512 = 31.25 = 32 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_default_mfcc_length(default_wav_duration=1):\n",
    "    length = int(math.ceil(default_wav_duration * default_sample_rate / default_hop_length))\n",
    "    return length\n",
    "\n",
    "def load_wav_mfcc(filename):\n",
    "    default_mfcc_length = get_default_mfcc_length()\n",
    "    #the base_mfcc is the (128, 32) vector\n",
    "    assert(base_mfcc.shape[1] == 32)\n",
    "    \n",
    "    out_mfcc = np.copy(base_mfcc)\n",
    "    wav_loader, sample_rate = rosa.load(filename, sr=default_sample_rate)\n",
    "    #print(rosa.get_duration(wav_loader, sample_rate))\n",
    "    wav_mfcc = rosa.feature.mfcc(y=wav_loader, sr=default_sample_rate, n_mfcc=default_number_of_mfcc)\n",
    "    out_mfcc[:, 0:wav_mfcc.shape[1]] = wav_mfcc    \n",
    "    out_mfcc = np.transpose(out_mfcc)\n",
    "    return out_mfcc\n",
    "\n",
    "def mfcc_display(mfccs):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    librosa.display.specshow(mfccs, x_axis='time')\n",
    "    plt.colorbar()\n",
    "    plt.title('MFCC')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "wav_mfcc_data = load_wav_mfcc(local_config.speech_data_dir + \"/six/f6a380e7_nohash_0.wav\")\n",
    "print(wav_mfcc_data.shape)\n",
    "mfcc_display(np.transpose(wav_mfcc_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav MFCC loader\n",
    "Wav file loader and export mfcc sequence. \n",
    "\n",
    "0, go throught all wav file to add background voice into command wav file\n",
    "1, go through all wav file and convert to MFCC sequence\n",
    "2, construct pair of MFCC sequence and a target (0 or 1, 0 for different command, 1 for the same command)\n",
    "    the same word * 1000, random generate key index, the first index of wav, and the second index of wav. \n",
    "    the diff word * 1000, random generae two key index, the first index of wav, and the second index of wav. \n",
    "    the format will be [mfcc 1, mfcc 2, 0/1 for the same or different]\n",
    "3, prepare pair of MFCC and targets according to batch size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class WavMFCCLoader(object):\n",
    "    def __init__(self, data_dir, wanted, validation_percentage=0, testing_percentage=0):\n",
    "        self.data_dir = data_dir\n",
    "        self.wanted = wanted\n",
    "        self.default_mfcc_length=get_default_mfcc_length(default_wav_duration)\n",
    "        self.wav_files = dict()\n",
    "        self.wav_file_index()\n",
    "        self.mfccs_mean = 0\n",
    "        self.mfccs_std = 0\n",
    "        \n",
    "        \n",
    "    def wav_file_index(self):\n",
    "        for dirpath, dirnames, files in os.walk(self.data_dir):\n",
    "            for name in files:\n",
    "                if name.lower().endswith('.wav'):\n",
    "                    word_name = dirpath.rsplit(local_config.default_split, 1)[1];\n",
    "                    if word_name in self.wanted:\n",
    "                        file_name = os.path.join(dirpath, name)\n",
    "                        #print(file_name, dirpath, word_name)\n",
    "    \n",
    "                        if word_name in self.wav_files.keys():\n",
    "                            self.wav_files[word_name].append(file_name)\n",
    "                        else:\n",
    "                            self.wav_files[word_name] = [file_name]\n",
    "                    \n",
    "        return self.wav_files\n",
    "    def mfcc_preprocess(self, mfcc_data_in, preprocess=\"l2-normalize\"):\n",
    "        mfcc_data_out = np.zeros(mfcc_data_in.shape)\n",
    "        if preprocess == \"global\":\n",
    "            self.get_sample_mean_std()\n",
    "            mfcc_data_out = (mfcc_data_in - self.mfccs_mean[0:mfcc_data_in.shape[0], :]) / self.mfccs_std[0:mfcc_data_in.shape[0], :]\n",
    "        elif preprocess == \"z-score\":\n",
    "            mfcc_data_out = preprocessing.scale(mfcc_data_in)\n",
    "        elif preprocess == \"max-min-scaler\":\n",
    "            mfcc_data_out = preprocessing.MinMaxScaler().fit_transform(mfcc_data_in)\n",
    "        elif preprocess == \"l2-normalize\":\n",
    "            mfcc_data_out = preprocessing.normalize(mfcc_data_in, norm='l2')\n",
    "        else:\n",
    "            raise ValueError(\"unknown proprocess\")  \n",
    "        return mfcc_data_out\n",
    "    #sequence size here means how many wavs compare to the anchor. \n",
    "    #the output will be anchor mfcc, the sequence mfcc and one-hot vector \n",
    "    #to indicate which one in the sequence is the same word as the anchor\n",
    "    def wavs_to_mfcc_sequence(self, local_preprocess=\"l2-normalize\", sequence_size=5):\n",
    "        how_many_words = len(self.wanted)\n",
    "        a_wav_index = b_wav_index = -1\n",
    "        #anchor mfcc\n",
    "        a_index = random.randint(0, how_many_words - 1) #anchor index\n",
    "        a_wav_index = random.randint(0, len(self.wav_files[self.wanted[a_index]]) - 1)\n",
    "        mfcc_anchor = load_wav_mfcc(self.wav_files[self.wanted[a_index]][a_wav_index])\n",
    "        mfcc_anchor = self.mfcc_preprocess(mfcc_anchor, preprocess=local_preprocess)\n",
    "        #sequence of mfccs\n",
    "        sequence_vec = np.zeros((sequence_size))\n",
    "        mfcc_seqs = np.zeros((sequence_size, mfcc_anchor.shape[0], mfcc_anchor.shape[1]))\n",
    "        #where to put the same word wav as anchor\n",
    "        set_one_pos = random.randint(0, sequence_size - 1) \n",
    "        for i in range(sequence_size):\n",
    "            if (i == set_one_pos):\n",
    "                b_wav_index = random.randint(0, len(self.wav_files[self.wanted[a_index]]) - 1)\n",
    "                mfcc_seq = load_wav_mfcc(self.wav_files[self.wanted[a_index]][b_wav_index])    \n",
    "                sequence_vec[i] = 1\n",
    "            else:\n",
    "                b_index = random.randint(0, how_many_words - 1)\n",
    "                while (a_index == b_index): # there has to be only one wav file with the same word as anchor\n",
    "                    b_index = random.randint(0, how_many_words - 1)\n",
    "                b_wav_index = random.randint(0, len(self.wav_files[self.wanted[b_index]]) - 1)\n",
    "                mfcc_seq = load_wav_mfcc(self.wav_files[self.wanted[b_index]][b_wav_index])\n",
    "                sequence_vec[i] = 0\n",
    "                \n",
    "            mfcc_seqs[i] = self.mfcc_preprocess(mfcc_seq, preprocess=local_preprocess)\n",
    "        #mfcc_seqs = np.transpose(mfcc_seqs, (1, 2, 0))            \n",
    "        #print(\"aaa\", mfcc_1.shape, mfcc_2.shape)    \n",
    "        return mfcc_anchor, mfcc_seqs, sequence_vec\n",
    "        \n",
    "    def get_mfcc_sequences(self, how_many, local_sequence_size = 5, preprocess='l2-normalize'):\n",
    "        mfcc_anchors = np.zeros((how_many, self.default_mfcc_length, default_number_of_mfcc))\n",
    "        mfcc_sequences = np.zeros((how_many, sequence_size, self.default_mfcc_length, default_number_of_mfcc))\n",
    "        sequence_vecs = np.zeros((how_many, sequence_size))\n",
    "        for i in range(how_many):\n",
    "            \n",
    "            mfcc_anchors[i], mfcc_sequences[i], sequence_vecs[i] = self.wavs_to_mfcc_sequence(sequence_size = local_sequence_size, local_preprocess = preprocess)\n",
    "            \n",
    "            \n",
    "    \n",
    "        return mfcc_anchors, mfcc_sequences, sequence_vecs\n",
    "    \n",
    "    def get_mfcc_for_word(self, wanted, local_preprocess='l2-normalize'):\n",
    "        mfcc_data = np.zeros((self.default_mfcc_length, default_number_of_mfcc))\n",
    "        if (wanted in self.wanted):\n",
    "            wav_index = random.randint(0, len(self.wav_files[wanted]) - 1)\n",
    "            mfcc_data_tmp = load_wav_mfcc(self.wav_files[wanted][wav_index])\n",
    "            mfcc_data_tmp = self.mfcc_preprocess(mfcc_data_tmp, preprocess=local_preprocess)\n",
    "            \n",
    "        \n",
    "            mfcc_data[0:mfcc_data_tmp.shape[0], : ] = mfcc_data_tmp\n",
    "        else:\n",
    "            raise ValueError(\"the word is not in the list\")\n",
    "            \n",
    "        return mfcc_data, self.wav_files[wanted][wav_index]\n",
    "        \n",
    "    def get_sample_mean_std(self):\n",
    "        count = 0\n",
    "        for i in (self.wanted):\n",
    "            #print(i)\n",
    "            for j in range(len(self.wav_files[i])):\n",
    "                #print(self.wav_files[i][j])\n",
    "                count += 1\n",
    "        #print(\"count:\", count)\n",
    "        mfccs = np.zeros((count, self.default_mfcc_length, default_number_of_mfcc))\n",
    "        index = 0\n",
    "        for i in (self.wanted):\n",
    "            for j in range(len(self.wav_files[i])):       \n",
    "                mfcc_ = load_wav_mfcc(self.wav_files[i][j])\n",
    "                mfccs[index, 0:mfcc_.shape[0], : ] = mfcc_\n",
    "                index += 1\n",
    "        self.mfccs_mean = mfccs.mean(0)\n",
    "        self.mfccs_std = mfccs.std(0)\n",
    "        \n",
    "\n",
    "#loader = WavMFCCLoader(local_config.speech_data_dir, wanted=[\"one\", \"two\", \"bed\", \"backward\", \"bird\", \"cat\", \"dog\", \"eight\", \"five\", \"follow\", \"forward\", \"four\", \"go\", \"happy\", \"house\", \"learn\", \"left\", \"marvin\", \"nine\", \"no\", \"off\", \"right\", \"seven\", \"sheila\", \"stop\", \"three\", \"tree\", \"visual\", \"wow\", \"zero\"])\n",
    "loader = WavMFCCLoader(local_config.speech_data_dir, wanted=[\"one\", \"two\", \"bed\", \"backward\"])\n",
    "sequence_size = 5\n",
    "#wav_list = loader.wav_file_index()\n",
    "mfcc_anchors, mfcc_sequences, sequence_vecs = loader.get_mfcc_sequences(3, local_sequence_size=sequence_size, preprocess='l2-normalize')\n",
    "for i in range(3):\n",
    "    mfcc_display(np.transpose(mfcc_anchors[i]))\n",
    "    print(\"the \" + str(i) + \" seq vec\")\n",
    "    for j in range(sequence_size):\n",
    "        mfcc_display(np.transpose(mfcc_sequences[i][j]))\n",
    "        print(sequence_vecs[i][j])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader for training and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_data_file(file_name):\n",
    "    f = open(file_name, \"rb\")\n",
    "    t = np.load(f)\n",
    "    #print(\"shape:\", t.shape)\n",
    "    f.close()\n",
    "    return t\n",
    "\n",
    "def write_data_file(file_name, data):\n",
    "    f = open(file_name, \"wb\")\n",
    "    np.save(f, data)\n",
    "    f.close()\n",
    "\n",
    "def save_model_file(model):\n",
    "    json_model = model.to_json()    \n",
    "    with open(local_config.default_processed_data_path + local_config.default_split + \"model_description.txt\", \"w\") as out_file:\n",
    "        out_file.write(json_model)\n",
    "        out_file.close()\n",
    "    plot_model(model, to_file=local_config.default_processed_data_path + local_config.default_split + 'model_description.png')\n",
    "    \n",
    "\n",
    "def store_processed_data(local_train_samples=default_train_samples, sequence_size = default_sequence_size,local_test_samples=default_test_samples, local_process_data_path=local_config.default_processed_data_path, local_preprocess='l2-normalize', local_train_wanted=default_train_wanted_words, local_test_wanted=default_test_wanted_words):\n",
    "    train_loader = WavMFCCLoader(local_config.speech_data_dir, wanted=local_train_wanted)\n",
    "    mfcc_train_anchors, mfcc_train_seqs, train_seq_vecs = train_loader.get_mfcc_sequences(local_train_samples, local_sequence_size = sequence_size, preprocess=local_preprocess)\n",
    "    mfcc_eval_anchors, mfcc_eval_seqs, eval_seq_vecs = train_loader.get_mfcc_sequences(local_test_samples, local_sequence_size = sequence_size, preprocess=local_preprocess)\n",
    "\n",
    "    test_loader  = WavMFCCLoader(local_config.speech_data_dir, wanted=local_test_wanted)\n",
    "    mfcc_test_anchors, mfcc_test_seqs, test_seq_vecs = test_loader.get_mfcc_sequences(local_test_samples, local_sequence_size = sequence_size, preprocess=local_preprocess)\n",
    "\n",
    "    #store the training mfcc anchor to np file\n",
    "    filename_train_mfcc_anchors = local_process_data_path + local_config.default_split + \"mfcc_train_anchors_\" + str(sequence_size) + '_' + str(local_train_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    write_data_file(filename_train_mfcc_anchors, mfcc_train_anchors)\n",
    "    \n",
    "    #store the training mfcc sequences to np file\n",
    "    filename_train_mfcc_sequences = local_process_data_path + local_config.default_split + \"mfcc_train_sequences_\" + str(sequence_size) + '_'  + str(local_train_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    write_data_file(filename_train_mfcc_sequences, mfcc_train_seqs)\n",
    "    \n",
    "    #store the training sequence vector data to np file\n",
    "    filename_train_seq_vecs = local_process_data_path + local_config.default_split + \"seq_vecs_train_\" + str(sequence_size) + '_'  + str(local_train_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    write_data_file(filename_train_seq_vecs, train_seq_vecs)\n",
    "    \n",
    "    #store the eval mfcc anchor to np file\n",
    "    filename_test_mfcc_anchors = local_process_data_path + local_config.default_split + \"mfcc_test_anchors_\" + str(sequence_size) + '_' + str(local_test_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    write_data_file(filename_test_mfcc_anchors, mfcc_test_anchors)\n",
    "    \n",
    "    #store the eval mfcc sequences to np file\n",
    "    filename_test_mfcc_sequences = local_process_data_path + local_config.default_split + \"mfcc_test_sequences_\" + str(sequence_size) + '_'  + str(local_test_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    write_data_file(filename_test_mfcc_sequences, mfcc_test_seqs)\n",
    "    \n",
    "    #store the eval sequence vector data to np file\n",
    "    filename_test_seq_vecs = local_process_data_path + local_config.default_split + \"seq_vecs_test_\" + str(sequence_size) + '_'  + str(local_test_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    write_data_file(filename_test_seq_vecs, test_seq_vecs)\n",
    "    \n",
    "    #store the eval mfcc anchor to np file\n",
    "    filename_eval_mfcc_anchors = local_process_data_path + local_config.default_split + \"mfcc_eval_anchors_\" + str(sequence_size) + '_' + str(local_test_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    write_data_file(filename_eval_mfcc_anchors, mfcc_eval_anchors)\n",
    "    \n",
    "    #store the eval mfcc sequences to np file\n",
    "    filename_eval_mfcc_sequences = local_process_data_path + local_config.default_split + \"mfcc_eval_sequences_\" + str(sequence_size) + '_'  + str(local_test_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    write_data_file(filename_eval_mfcc_sequences, mfcc_eval_seqs)\n",
    "    \n",
    "    #store the eval sequence vector data to np file\n",
    "    filename_eval_seq_vecs = local_process_data_path + local_config.default_split + \"seq_vecs_eval_\" + str(sequence_size) + '_'  + str(local_test_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    write_data_file(filename_eval_seq_vecs, eval_seq_vecs)\n",
    "    \n",
    "    \n",
    "def load_processed_data(local_train_samples=default_train_samples, local_test_samples=default_test_samples, local_process_data_path=local_config.default_processed_data_path, sequence_size = default_sequence_size, local_preprocess='l2-normalize'):\n",
    "    \n",
    "    filename_train_mfcc_anchor = local_process_data_path + local_config.default_split + \"mfcc_train_anchors_\" + str(sequence_size) + '_'  + str(local_train_samples) + \"_\" + local_preprocess + \".npy\"    \n",
    "    \n",
    "    if not (os.path.isfile(filename_train_mfcc_anchor)):\n",
    "        store_processed_data(local_train_samples, sequence_size, local_test_samples, local_process_data_path, local_preprocess)\n",
    "    \n",
    "    filename_train_mfcc_anchors = local_process_data_path + local_config.default_split + \"mfcc_train_anchors_\" + str(sequence_size) + '_' + str(local_train_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    mfcc_train_anchors = read_data_file(filename_train_mfcc_anchors)\n",
    "    filename_train_mfcc_sequences = local_process_data_path + local_config.default_split + \"mfcc_train_sequences_\" + str(sequence_size) + '_' + str(local_train_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    mfcc_train_sequences = read_data_file(filename_train_mfcc_sequences)\n",
    "    filename_train_seq_vecs = local_process_data_path + local_config.default_split + \"seq_vecs_train_\" + str(sequence_size) + '_' + str(local_train_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    train_seq_vecs = read_data_file(filename_train_seq_vecs)\n",
    "    filename_eval_mfcc_anchors = local_process_data_path + local_config.default_split + \"mfcc_eval_anchors_\" + str(sequence_size) + '_' + str(local_test_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    mfcc_eval_anchors = read_data_file(filename_eval_mfcc_anchors)\n",
    "    filename_eval_mfcc_sequences = local_process_data_path + local_config.default_split + \"mfcc_eval_sequences_\" + str(sequence_size) + '_' + str(local_test_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    mfcc_eval_sequences = read_data_file(filename_eval_mfcc_sequences)\n",
    "    filename_eval_seq_vecs = local_process_data_path + local_config.default_split + \"seq_vecs_eval_\" + str(sequence_size) + '_' + str(local_test_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    eval_seq_vecs = read_data_file(filename_eval_seq_vecs)\n",
    "    filename_test_mfcc_anchors = local_process_data_path + local_config.default_split + \"mfcc_test_anchors_\" + str(sequence_size) + '_' + str(local_test_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    mfcc_test_anchors = read_data_file(filename_test_mfcc_anchors)\n",
    "    filename_test_mfcc_sequences = local_process_data_path + local_config.default_split + \"mfcc_test_sequences_\" + str(sequence_size) + '_' + str(local_test_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    mfcc_test_sequences = read_data_file(filename_test_mfcc_sequences)\n",
    "    filename_test_seq_vecs = local_process_data_path + local_config.default_split + \"seq_vecs_test_\" + str(sequence_size) + '_' + str(local_test_samples) + \"_\" + local_preprocess + \".npy\"\n",
    "    test_seq_vecs = read_data_file(filename_test_seq_vecs)    \n",
    "    \n",
    "    return mfcc_train_anchors, mfcc_train_sequences, train_seq_vecs, mfcc_eval_anchors, mfcc_eval_sequences, eval_seq_vecs, mfcc_test_anchors, mfcc_test_sequences, test_seq_vecs\n",
    "\n",
    "#load_processed_data(local_train_samples=10000, local_test_samples=100, local_preprocess='z-score')\n",
    "_=load_processed_data(local_train_samples=10000, sequence_size = 5, local_test_samples=100, local_preprocess='l2-normalize')\n",
    "#load_processed_data(local_train_samples=10000, local_test_samples=100, local_preprocess='global')\n",
    "#load_processed_data(local_train_samples=10000, local_test_samples=100, local_preprocess='max-min-scaler')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a keras lstm network, take mfcc vector as input.\n",
    "\n",
    "the speech command mfcc input shape is (?, mfcc_number, hop_number, 1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(local_input_shape, is_training=True):\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Dense(128, kernel_regularizer=default_kernel_regularizer, activity_regularizer=default_activity_regularizer, name='time_dense'), input_shape=local_input_shape))\n",
    "    model.add(GRU(128, return_sequences=False, stateful=False))\n",
    "    #model.add(GRU(256, return_sequences=True, stateful=False))\n",
    "    #model.add(GRU(256, stateful=False))\n",
    "\n",
    "    model.add(Dense(128, kernel_regularizer=default_kernel_regularizer, activity_regularizer=default_activity_regularizer, name='after_gru_dense'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\")) \n",
    "    #if (is_training):\n",
    "    #    model.add(Dropout(0.5))\n",
    "    #model.add(Dense(labels_count, activation=\"softmax\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def euclidean_distance(x):     \n",
    "    ret = K.mean( K.square(x[0]-x[1]),axis=-1,keepdims=True)\n",
    "    #print(ret.shape)\n",
    "    return ret #K.mean( K.square(x1-x2),axis=-1,keepdims=True)\n",
    "    #return distance.euclidean(x[0], x[1])\n",
    "\n",
    "def cosine_distance(x):        \n",
    "    x1 = K.l2_normalize(x[0], axis=-1)\n",
    "    x2 = K.l2_normalize(x[1], axis=-1)\n",
    "    return -K.mean(x1 * x2, axis=-1, keepdims=True)\n",
    "\n",
    "def create_siamese_model(input_shape, sequence_size = default_sequence_size, siamese_mode = 'abs'):\n",
    "    \n",
    "    anchor_input = Input(input_shape, name='anchor_input')\n",
    "    keras_model = create_lstm_model(input_shape)\n",
    "    anchor_encoder = keras_model(anchor_input)\n",
    "    seq_inputs = [anchor_input]\n",
    "    seq_inputs_encoder = []\n",
    "    distance_vector = []\n",
    "    for i in range(sequence_size):\n",
    "        seq_input = Input(input_shape, name='seq'+str(i)+'_input')\n",
    "        seq_encoder = keras_model(seq_input)\n",
    "        if (siamese_mode == 'dot'):\n",
    "            calculate_layer = Lambda(lambda x: x[0]*x[1], name='lambda_dot'+str(i))([anchor_encoder, seq_encoder])\n",
    "        elif (siamese_mode == 'abs'):\n",
    "            calculate_layer = Lambda(lambda x: tf.abs(x[0]-x[1], name='lambda_abs'+str(i)))([anchor_encoder, seq_encoder])\n",
    "        elif (siamese_mode == 'euclidean'):\n",
    "            calculate_layer = Lambda(euclidean_distance, name='lambda_euclidean'+str(i))([anchor_encoder, seq_encoder])\n",
    "        elif (siamese_mode == 'cosine'):\n",
    "            calculate_layer = Lambda(cosine_distance, name='lambda_cosine'+str(i))([anchor_encoder, seq_encoder])\n",
    "        else:\n",
    "            raise ValueError(\"unknown siamese_mode\")   \n",
    "        seq_inputs.append(seq_input)\n",
    "        seq_inputs_encoder.append(calculate_layer)\n",
    "        \n",
    "   \n",
    "    concatenated_layer = concatenate(inputs=seq_inputs_encoder)\n",
    "    \n",
    "        \n",
    "    if (siamese_mode == 'dot') or (siamese_mode == 'abs'):     \n",
    "        last_hiden_layer = Dense(512, activation='relu', kernel_regularizer=default_kernel_regularizer, activity_regularizer=default_activity_regularizer, name='dense_512')(concatenated_layer)\n",
    "    else:\n",
    "        last_hiden_layer = concatenated_layer\n",
    "    output_layer = Dense(sequence_size, activation='softmax', kernel_regularizer=default_kernel_regularizer, activity_regularizer=default_activity_regularizer, name='dense_output')(last_hiden_layer)\n",
    "    \n",
    "    siamese_model = Model(seq_inputs, output_layer)\n",
    "    return siamese_model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopOnBaseline(Callback):\n",
    "    \"\"\"Callback that terminates training when either acc or val_acc reaches a specified baseline\n",
    "    \"\"\"\n",
    "    def __init__(self, monitor='val_acc', baseline=0.9, patience=0):\n",
    "        super(EarlyStopOnBaseline, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.baseline = baseline\n",
    "        self.patience = patience\n",
    "        self.count_for_patience = 0\n",
    "        self.count_for_best = 0\n",
    "        self.best_acc = 0\n",
    "        self.torlerence = default_improving_torlerence\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        acc = logs.get(self.monitor)\n",
    "        #print('Epoch %d: EarlyStopOnBaseline call back'%epoch)\n",
    "        if acc is not None:\n",
    "            if acc >= self.baseline:\n",
    "                self.count_for_patience = self.count_for_patience + 1\n",
    "                if (self.count_for_patience >= self.patience):\n",
    "                    print('Epoch %d: Reached baseline, terminating training' % (epoch))\n",
    "                    self.model.stop_training = True\n",
    "            elif acc > self.best_acc:\n",
    "                self.best_acc = acc\n",
    "                self.count_for_best = 0 \n",
    "                #print(self.best_acc)\n",
    "            elif acc <= self.best_acc:\n",
    "                self.count_for_best = self.count_for_best + 1\n",
    "                #print(self.count_for_best)\n",
    "                if self.count_for_best >= self.torlerence:\n",
    "                    print('Epoch %d: No improving for long time, terminating training' % (epoch))\n",
    "                    self.model.stop_training = True\n",
    "\n",
    "def siamese_train(mfcc_train_anchors, mfcc_train_sequences, train_seq_vecs, mfcc_eval_anchors, mfcc_eval_sequences, eval_seq_vecs, local_sequence_size=default_sequence_size,local_siamese_mode='abs', local_batch_size=default_batch_size, local_epochs= default_epochs, local_optimizer='adam'):\n",
    "    default_mfcc_length = get_default_mfcc_length(default_wav_duration)\n",
    "    siamese_model = create_siamese_model((default_mfcc_length, default_number_of_mfcc), sequence_size=local_sequence_size, siamese_mode=local_siamese_mode)\n",
    "    #save_model_file(siamese_model)\n",
    "    #siamese_model = multi_gpu_model(siamese_model, gpus=2)\n",
    "    #siamese_model.summary()\n",
    "    siamese_model.compile(loss='categorical_crossentropy', optimizer=local_optimizer, metrics=['accuracy'])\n",
    "    x_train_inputs=[mfcc_train_anchors]\n",
    "    mfcc_train_sequences = np.transpose(mfcc_train_sequences, (1, 0, 2, 3))\n",
    "    for i in range(sequence_size): \n",
    "        \n",
    "        #print(mfcc_train_sequences.shape)\n",
    "        x_train_inputs.append(mfcc_train_sequences[i])\n",
    "    y_train = train_seq_vecs\n",
    "\n",
    "    #callbacks , , histogram_freq=32\n",
    "    tensorboard = keras.callbacks.TensorBoard(log_dir=local_config.default_processed_data_path+'/logs', write_graph=True, write_grads=True, write_images=True)\n",
    "    earlystop = EarlyStopOnBaseline(monitor='val_acc',baseline=0.8)\n",
    "    #modelchecker = keras.callbacks.ModelCheckpoint(local_config.default_model_path, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "    #print(\"x_train_inputs \" + str(x_train_inputs[0].shape) + \" y_train \" + str(y_train))\n",
    "    siamese_model.fit(x_train_inputs, y_train, epochs=local_epochs, batch_size=local_batch_size, callbacks=[tensorboard, earlystop], validation_split=0.2) #, earlystop\n",
    "    \n",
    "    \n",
    "    x_eval_inputs=[mfcc_eval_anchors]\n",
    "    mfcc_eval_sequences = np.transpose(mfcc_eval_sequences, (1, 0, 2, 3))\n",
    "    for i in range(sequence_size):        \n",
    "        x_eval_inputs.append(mfcc_eval_sequences[i])\n",
    "    y_eval = eval_seq_vecs\n",
    "\n",
    "    \n",
    "    loss, accuracy = siamese_model.evaluate(x_eval_inputs, y_eval)    \n",
    "    print(str(loss)+'  ' +str(accuracy))\n",
    "    siamese_model.save(local_config.default_model_path+\"/speech_siamese.h5\")\n",
    "\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def siamese_test(mfcc_test_anchors, mfcc_test_sequences, test_seq_vecs):\n",
    "    default_mfcc_length = get_default_mfcc_length(default_wav_duration)    \n",
    "    siamese_model = keras.models.load_model(local_config.default_model_path+\"/speech_siamese.h5\")    \n",
    "\n",
    "    x_test_inputs=[mfcc_test_anchors]\n",
    "    mfcc_test_sequences = np.transpose(mfcc_test_sequences, (1, 0, 2, 3))\n",
    "    for i in range(sequence_size):\n",
    "        \n",
    "        x_test_inputs.append(mfcc_test_sequences[i])\n",
    "    y_test = test_seq_vecs\n",
    "    \n",
    "    loss, accuracy = siamese_model.test_on_batch(x=x_test_inputs, y=y_test)\n",
    "    print(loss)\n",
    "    \n",
    "    #keep the accurcy beyond .75 to a separated file\n",
    "    if accuracy > .75:\n",
    "        good_model_filename = local_config.default_model_path+ \"/speech_siamese_\"+str(int(accuracy * 100))+\".h5\"\n",
    "        shutil.copyfile(local_config.default_model_path+\"/speech_siamese.h5\", good_model_filename)\n",
    "        \n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#wav_mfcc = load_wav_mfcc(\"/Users/hermitwang/Downloads/speech_dataset/backward/0a2b400e_nohash_0.wav\")\n",
    "#print(wav_mfcc.shape) \n",
    "train_samples = default_train_samples #* 5\n",
    "test_samples = default_test_samples\n",
    "batch_size_numbers=[128, 256, 512, 1024, 2048]\n",
    "optimizers = ['adam', 'nadam', 'adamax']\n",
    "siamese_modes = ['abs', 'dot' ]\n",
    "train_epochs = 500\n",
    "print(\"Start at \" + str(datetime.datetime.now()))\n",
    "mfcc_train_anchors, mfcc_train_sequences, train_seq_vecs, mfcc_eval_anchors, mfcc_eval_sequences, eval_seq_vecs, mfcc_test_anchors, mfcc_test_sequences, test_seq_vecs = load_processed_data(local_train_samples=train_samples, sequence_size = 5, local_test_samples=test_samples, local_preprocess='l2-normalize')\n",
    "\n",
    "for i in range(10):\n",
    "    j = 3\n",
    "    for m in range(len(optimizers)):\n",
    "        for n in range(len(siamese_modes)):\n",
    "            print(\"Train batch size \" + str(batch_size_numbers[j]) + \" of \" + str(i+1) + \" run with optimizer \" + optimizers[m] + \" and siamese mode \"+siamese_modes[n]+\" start at \" + str(datetime.datetime.now()))\n",
    "            score=siamese_train(mfcc_train_anchors, mfcc_train_sequences, train_seq_vecs, mfcc_eval_anchors, mfcc_eval_sequences, eval_seq_vecs, local_sequence_size=default_sequence_size, local_siamese_mode=siamese_modes[n], local_batch_size=batch_size_numbers[j], local_epochs=train_epochs, local_optimizer=optimizers[m])\n",
    "            print(score)\n",
    "            score=siamese_test(mfcc_test_anchors, mfcc_test_sequences, test_seq_vecs)\n",
    "    \n",
    "            print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on group data \n",
    "\n",
    "the real case to use the model is compare the input to all prebuilt command. so if everytime the matched command get the highest possibility, the model is feasible enough.\n",
    "\n",
    "the follow code is trying to evaluate the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# siamese mode abs and dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "siamese mode: abs\n",
    "\n",
    "Start at 2018-12-18 19:59:19.594174\n",
    "Train batch size 1024 of 1 run start at 2018-12-18 19:59:20.364893:\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "anchor_input (InputLayer)       (None, 32, 128)      0                                            \n",
    "__________________________________________________________________________________________________\n",
    "seq0_input (InputLayer)         (None, 32, 128)      0                                            \n",
    "__________________________________________________________________________________________________\n",
    "seq1_input (InputLayer)         (None, 32, 128)      0                                            \n",
    "__________________________________________________________________________________________________\n",
    "seq2_input (InputLayer)         (None, 32, 128)      0                                            \n",
    "__________________________________________________________________________________________________\n",
    "seq3_input (InputLayer)         (None, 32, 128)      0                                            \n",
    "__________________________________________________________________________________________________\n",
    "seq4_input (InputLayer)         (None, 32, 128)      0                                            \n",
    "__________________________________________________________________________________________________\n",
    "sequential (Sequential)         (None, 128)          132224      anchor_input[0][0]               \n",
    "                                                                 seq0_input[0][0]                 \n",
    "                                                                 seq1_input[0][0]                 \n",
    "                                                                 seq2_input[0][0]                 \n",
    "                                                                 seq3_input[0][0]                 \n",
    "                                                                 seq4_input[0][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "lambda (Lambda)                 (None, 128)          0           sequential[1][0]                 \n",
    "                                                                 sequential[2][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "lambda_1 (Lambda)               (None, 128)          0           sequential[1][0]                 \n",
    "                                                                 sequential[3][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "lambda_2 (Lambda)               (None, 128)          0           sequential[1][0]                 \n",
    "                                                                 sequential[4][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "lambda_3 (Lambda)               (None, 128)          0           sequential[1][0]                 \n",
    "                                                                 sequential[5][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "lambda_4 (Lambda)               (None, 128)          0           sequential[1][0]                 \n",
    "                                                                 sequential[6][0]                 \n",
    "__________________________________________________________________________________________________\n",
    "concatenate (Concatenate)       (None, 640)          0           lambda[0][0]                     \n",
    "                                                                 lambda_1[0][0]                   \n",
    "                                                                 lambda_2[0][0]                   \n",
    "                                                                 lambda_3[0][0]                   \n",
    "                                                                 lambda_4[0][0]                   \n",
    "__________________________________________________________________________________________________\n",
    "dense_512 (Dense)               (None, 512)          328192      concatenate[0][0]                \n",
    "__________________________________________________________________________________________________\n",
    "dense_output (Dense)            (None, 5)            2565        dense_512[0][0]                  \n",
    "==================================================================================================\n",
    "Total params: 462,981\n",
    "Trainable params: 462,725\n",
    "Non-trainable params: 256\n",
    "__________________________________________________________________________________________________\n",
    "Train on 8000 samples, validate on 2000 samples\n",
    "Epoch 1/100\n",
    "8000/8000 [==============================] - 8s 1ms/step - loss: 8.7885 - acc: 0.2041 - val_loss: 7.4398 - val_acc: 0.2015\n",
    "Epoch 2/100\n",
    "8000/8000 [==============================] - 4s 470us/step - loss: 6.7969 - acc: 0.2260 - val_loss: 6.0289 - val_acc: 0.2055\n",
    "Epoch 3/100\n",
    "8000/8000 [==============================] - 4s 455us/step - loss: 5.5353 - acc: 0.2469 - val_loss: 4.9495 - val_acc: 0.2095\n",
    "Epoch 4/100\n",
    "8000/8000 [==============================] - 4s 454us/step - loss: 4.5581 - acc: 0.2451 - val_loss: 4.1019 - val_acc: 0.1985\n",
    "Epoch 5/100\n",
    "8000/8000 [==============================] - 4s 463us/step - loss: 3.7969 - acc: 0.2361 - val_loss: 3.4441 - val_acc: 0.2070\n",
    "Epoch 6/100\n",
    "8000/8000 [==============================] - 4s 462us/step - loss: 3.2052 - acc: 0.2465 - val_loss: 2.9408 - val_acc: 0.2050\n",
    "Epoch 7/100\n",
    "8000/8000 [==============================] - 4s 469us/step - loss: 2.7643 - acc: 0.2420 - val_loss: 2.5683 - val_acc: 0.1955\n",
    "Epoch 8/100\n",
    "8000/8000 [==============================] - 4s 460us/step - loss: 2.4387 - acc: 0.2399 - val_loss: 2.2987 - val_acc: 0.2020\n",
    "Epoch 9/100\n",
    "8000/8000 [==============================] - 4s 464us/step - loss: 2.2053 - acc: 0.2338 - val_loss: 2.1059 - val_acc: 0.2095\n",
    "Epoch 10/100\n",
    "8000/8000 [==============================] - 4s 464us/step - loss: 2.0406 - acc: 0.2156 - val_loss: 1.9684 - val_acc: 0.2140\n",
    "Epoch 11/100\n",
    "8000/8000 [==============================] - 4s 466us/step - loss: 1.9215 - acc: 0.2288 - val_loss: 1.8707 - val_acc: 0.2110\n",
    "Epoch 12/100\n",
    "8000/8000 [==============================] - 4s 460us/step - loss: 1.8334 - acc: 0.2349 - val_loss: 1.7993 - val_acc: 0.2405\n",
    "Epoch 13/100\n",
    "8000/8000 [==============================] - 4s 466us/step - loss: 1.7704 - acc: 0.2535 - val_loss: 1.7409 - val_acc: 0.2585\n",
    "Epoch 14/100\n",
    "8000/8000 [==============================] - 4s 462us/step - loss: 1.6972 - acc: 0.2840 - val_loss: 1.6573 - val_acc: 0.3070\n",
    "Epoch 15/100\n",
    "8000/8000 [==============================] - 4s 444us/step - loss: 1.6398 - acc: 0.3040 - val_loss: 1.6329 - val_acc: 0.3000\n",
    "Epoch 16/100\n",
    "8000/8000 [==============================] - 4s 461us/step - loss: 1.6009 - acc: 0.3179 - val_loss: 1.6026 - val_acc: 0.3235\n",
    "Epoch 17/100\n",
    "8000/8000 [==============================] - 4s 448us/step - loss: 1.5853 - acc: 0.3275 - val_loss: 1.6148 - val_acc: 0.3245\n",
    "Epoch 18/100\n",
    "8000/8000 [==============================] - 3s 437us/step - loss: 1.5493 - acc: 0.3419 - val_loss: 1.6030 - val_acc: 0.3110\n",
    "Epoch 19/100\n",
    "8000/8000 [==============================] - 4s 458us/step - loss: 1.5386 - acc: 0.3440 - val_loss: 1.5772 - val_acc: 0.3190\n",
    "Epoch 20/100\n",
    "8000/8000 [==============================] - 4s 460us/step - loss: 1.5350 - acc: 0.3455 - val_loss: 1.6079 - val_acc: 0.3090\n",
    "Epoch 21/100\n",
    "8000/8000 [==============================] - 4s 460us/step - loss: 1.5255 - acc: 0.3491 - val_loss: 1.5841 - val_acc: 0.3355\n",
    "Epoch 22/100\n",
    "8000/8000 [==============================] - 4s 465us/step - loss: 1.5207 - acc: 0.3515 - val_loss: 1.5829 - val_acc: 0.3260\n",
    "Epoch 23/100\n",
    "8000/8000 [==============================] - 4s 463us/step - loss: 1.5109 - acc: 0.3534 - val_loss: 1.5835 - val_acc: 0.3240\n",
    "Epoch 24/100\n",
    "8000/8000 [==============================] - 4s 462us/step - loss: 1.5146 - acc: 0.3575 - val_loss: 1.5696 - val_acc: 0.3160\n",
    "Epoch 25/100\n",
    "8000/8000 [==============================] - 4s 467us/step - loss: 1.5089 - acc: 0.3515 - val_loss: 1.5706 - val_acc: 0.3250\n",
    "Epoch 26/100\n",
    "8000/8000 [==============================] - 4s 454us/step - loss: 1.5003 - acc: 0.3700 - val_loss: 1.5799 - val_acc: 0.3365\n",
    "Epoch 27/100\n",
    "8000/8000 [==============================] - 4s 462us/step - loss: 1.5000 - acc: 0.3602 - val_loss: 1.5167 - val_acc: 0.3520\n",
    "Epoch 28/100\n",
    "8000/8000 [==============================] - 4s 459us/step - loss: 1.4883 - acc: 0.3678 - val_loss: 1.6139 - val_acc: 0.3020\n",
    "Epoch 29/100\n",
    "8000/8000 [==============================] - 4s 469us/step - loss: 1.4940 - acc: 0.3610 - val_loss: 1.5333 - val_acc: 0.3310\n",
    "Epoch 30/100\n",
    "8000/8000 [==============================] - 4s 468us/step - loss: 1.4799 - acc: 0.3737 - val_loss: 1.5885 - val_acc: 0.3240\n",
    "Epoch 31/100\n",
    "8000/8000 [==============================] - 4s 453us/step - loss: 1.4781 - acc: 0.3670 - val_loss: 1.5658 - val_acc: 0.3245\n",
    "Epoch 32/100\n",
    "8000/8000 [==============================] - 4s 461us/step - loss: 1.4900 - acc: 0.3634 - val_loss: 1.5249 - val_acc: 0.3565\n",
    "Epoch 33/100\n",
    "8000/8000 [==============================] - 4s 459us/step - loss: 1.4744 - acc: 0.3811 - val_loss: 1.5313 - val_acc: 0.3485\n",
    "Epoch 34/100\n",
    "8000/8000 [==============================] - 4s 464us/step - loss: 1.4754 - acc: 0.3811 - val_loss: 1.5567 - val_acc: 0.3340\n",
    "Epoch 35/100\n",
    "8000/8000 [==============================] - 4s 469us/step - loss: 1.4680 - acc: 0.3806 - val_loss: 1.5900 - val_acc: 0.3055\n",
    "Epoch 36/100\n",
    "8000/8000 [==============================] - 4s 456us/step - loss: 1.4695 - acc: 0.3810 - val_loss: 1.5284 - val_acc: 0.3715\n",
    "Epoch 37/100\n",
    "8000/8000 [==============================] - 4s 458us/step - loss: 1.4510 - acc: 0.3965 - val_loss: 1.4816 - val_acc: 0.3735\n",
    "Epoch 38/100\n",
    "8000/8000 [==============================] - 4s 456us/step - loss: 1.4555 - acc: 0.3943 - val_loss: 1.4506 - val_acc: 0.3920\n",
    "Epoch 39/100\n",
    "8000/8000 [==============================] - 4s 459us/step - loss: 1.4444 - acc: 0.4025 - val_loss: 1.4953 - val_acc: 0.3675\n",
    "Epoch 40/100\n",
    "8000/8000 [==============================] - 4s 458us/step - loss: 1.4472 - acc: 0.3997 - val_loss: 1.4724 - val_acc: 0.3795\n",
    "Epoch 41/100\n",
    "8000/8000 [==============================] - 4s 456us/step - loss: 1.4094 - acc: 0.4235 - val_loss: 1.4512 - val_acc: 0.3990\n",
    "Epoch 42/100\n",
    "8000/8000 [==============================] - 4s 457us/step - loss: 1.4032 - acc: 0.4280 - val_loss: 1.4430 - val_acc: 0.3995\n",
    "Epoch 43/100\n",
    "8000/8000 [==============================] - 4s 459us/step - loss: 1.3861 - acc: 0.4371 - val_loss: 1.3884 - val_acc: 0.4355\n",
    "Epoch 44/100\n",
    "8000/8000 [==============================] - 4s 457us/step - loss: 1.3681 - acc: 0.4494 - val_loss: 1.3686 - val_acc: 0.4495\n",
    "Epoch 45/100\n",
    "8000/8000 [==============================] - 4s 455us/step - loss: 1.3459 - acc: 0.4704 - val_loss: 1.3893 - val_acc: 0.4380\n",
    "Epoch 46/100\n",
    "8000/8000 [==============================] - 4s 466us/step - loss: 1.3286 - acc: 0.4711 - val_loss: 1.3520 - val_acc: 0.4565\n",
    "Epoch 47/100\n",
    "8000/8000 [==============================] - 4s 466us/step - loss: 1.3226 - acc: 0.4759 - val_loss: 1.3741 - val_acc: 0.4560\n",
    "Epoch 48/100\n",
    "8000/8000 [==============================] - 4s 468us/step - loss: 1.3032 - acc: 0.4886 - val_loss: 1.3817 - val_acc: 0.4520\n",
    "Epoch 49/100\n",
    "8000/8000 [==============================] - 4s 468us/step - loss: 1.2955 - acc: 0.4926 - val_loss: 1.3178 - val_acc: 0.4825\n",
    "Epoch 50/100\n",
    "8000/8000 [==============================] - 4s 460us/step - loss: 1.2784 - acc: 0.5064 - val_loss: 1.3889 - val_acc: 0.4460\n",
    "Epoch 51/100\n",
    "8000/8000 [==============================] - 4s 466us/step - loss: 1.2688 - acc: 0.5110 - val_loss: 1.2953 - val_acc: 0.4890\n",
    "Epoch 52/100\n",
    "8000/8000 [==============================] - 4s 459us/step - loss: 1.2570 - acc: 0.5164 - val_loss: 1.3195 - val_acc: 0.4700\n",
    "Epoch 53/100\n",
    "8000/8000 [==============================] - 4s 455us/step - loss: 1.2416 - acc: 0.5271 - val_loss: 1.2476 - val_acc: 0.5220\n",
    "Epoch 54/100\n",
    "8000/8000 [==============================] - 4s 461us/step - loss: 1.2286 - acc: 0.5373 - val_loss: 1.2951 - val_acc: 0.4900\n",
    "Epoch 55/100\n",
    "8000/8000 [==============================] - 4s 458us/step - loss: 1.1924 - acc: 0.5570 - val_loss: 1.2766 - val_acc: 0.5015\n",
    "Epoch 56/100\n",
    "8000/8000 [==============================] - 4s 462us/step - loss: 1.2035 - acc: 0.5545 - val_loss: 1.2285 - val_acc: 0.5340\n",
    "Epoch 57/100\n",
    "8000/8000 [==============================] - 4s 455us/step - loss: 1.1657 - acc: 0.5739 - val_loss: 1.3054 - val_acc: 0.4990\n",
    "Epoch 58/100\n",
    "8000/8000 [==============================] - 4s 459us/step - loss: 1.1601 - acc: 0.5776 - val_loss: 1.2178 - val_acc: 0.5375\n",
    "Epoch 59/100\n",
    "8000/8000 [==============================] - 4s 466us/step - loss: 1.1500 - acc: 0.5762 - val_loss: 1.2132 - val_acc: 0.5450\n",
    "Epoch 60/100\n",
    "8000/8000 [==============================] - 4s 463us/step - loss: 1.1244 - acc: 0.5950 - val_loss: 1.1985 - val_acc: 0.5505\n",
    "Epoch 61/100\n",
    "8000/8000 [==============================] - 4s 466us/step - loss: 1.1214 - acc: 0.5920 - val_loss: 1.1786 - val_acc: 0.5720\n",
    "Epoch 62/100\n",
    "8000/8000 [==============================] - 4s 469us/step - loss: 1.1056 - acc: 0.6100 - val_loss: 1.1585 - val_acc: 0.5725\n",
    "Epoch 63/100\n",
    "8000/8000 [==============================] - 4s 475us/step - loss: 1.0828 - acc: 0.6126 - val_loss: 1.2358 - val_acc: 0.5585\n",
    "Epoch 64/100\n",
    "8000/8000 [==============================] - 4s 474us/step - loss: 1.0678 - acc: 0.6226 - val_loss: 1.2021 - val_acc: 0.5700\n",
    "Epoch 65/100\n",
    "8000/8000 [==============================] - 4s 468us/step - loss: 1.0711 - acc: 0.6216 - val_loss: 1.1817 - val_acc: 0.5845\n",
    "Epoch 66/100\n",
    "8000/8000 [==============================] - 4s 465us/step - loss: 1.0703 - acc: 0.6199 - val_loss: 1.2144 - val_acc: 0.5730\n",
    "Epoch 67/100\n",
    "8000/8000 [==============================] - 4s 464us/step - loss: 1.0316 - acc: 0.6500 - val_loss: 1.1518 - val_acc: 0.5825\n",
    "Epoch 68/100\n",
    "8000/8000 [==============================] - 4s 468us/step - loss: 1.0159 - acc: 0.6489 - val_loss: 1.1887 - val_acc: 0.5765\n",
    "Epoch 69/100\n",
    "8000/8000 [==============================] - 4s 468us/step - loss: 1.0365 - acc: 0.6482 - val_loss: 1.1541 - val_acc: 0.5900\n",
    "Epoch 70/100\n",
    "8000/8000 [==============================] - 4s 467us/step - loss: 1.0152 - acc: 0.6589 - val_loss: 1.1209 - val_acc: 0.6095\n",
    "Epoch 71/100\n",
    "8000/8000 [==============================] - 4s 472us/step - loss: 0.9859 - acc: 0.6684 - val_loss: 1.1416 - val_acc: 0.5995\n",
    "Epoch 72/100\n",
    "8000/8000 [==============================] - 4s 467us/step - loss: 0.9886 - acc: 0.6635 - val_loss: 1.0997 - val_acc: 0.6055\n",
    "Epoch 73/100\n",
    "8000/8000 [==============================] - 4s 465us/step - loss: 0.9779 - acc: 0.6735 - val_loss: 1.1311 - val_acc: 0.6040\n",
    "Epoch 74/100\n",
    "8000/8000 [==============================] - 4s 458us/step - loss: 0.9715 - acc: 0.6762 - val_loss: 1.1223 - val_acc: 0.6120\n",
    "Epoch 75/100\n",
    "8000/8000 [==============================] - 4s 458us/step - loss: 0.9513 - acc: 0.6858 - val_loss: 1.1395 - val_acc: 0.5975\n",
    "Epoch 76/100\n",
    "8000/8000 [==============================] - 4s 464us/step - loss: 0.9561 - acc: 0.6819 - val_loss: 1.0968 - val_acc: 0.6280\n",
    "Epoch 77/100\n",
    "8000/8000 [==============================] - 4s 465us/step - loss: 0.9386 - acc: 0.6955 - val_loss: 1.0971 - val_acc: 0.6120\n",
    "Epoch 78/100\n",
    "8000/8000 [==============================] - 4s 466us/step - loss: 0.9248 - acc: 0.7054 - val_loss: 1.0546 - val_acc: 0.6500\n",
    "Epoch 79/100\n",
    "8000/8000 [==============================] - 4s 462us/step - loss: 0.9217 - acc: 0.7030 - val_loss: 1.0317 - val_acc: 0.6565\n",
    "Epoch 80/100\n",
    "8000/8000 [==============================] - 4s 467us/step - loss: 0.9028 - acc: 0.7112 - val_loss: 1.0539 - val_acc: 0.6435\n",
    "Epoch 81/100\n",
    "8000/8000 [==============================] - 4s 463us/step - loss: 0.8822 - acc: 0.7229 - val_loss: 1.1169 - val_acc: 0.6255\n",
    "Epoch 82/100\n",
    "8000/8000 [==============================] - 4s 464us/step - loss: 0.8788 - acc: 0.7204 - val_loss: 1.0404 - val_acc: 0.6420\n",
    "Epoch 83/100\n",
    "8000/8000 [==============================] - 4s 462us/step - loss: 0.8876 - acc: 0.7182 - val_loss: 1.0103 - val_acc: 0.6550\n",
    "Epoch 84/100\n",
    "8000/8000 [==============================] - 4s 463us/step - loss: 0.8603 - acc: 0.7297 - val_loss: 0.9879 - val_acc: 0.6760\n",
    "Epoch 85/100\n",
    "8000/8000 [==============================] - 4s 460us/step - loss: 0.8375 - acc: 0.7446 - val_loss: 1.1248 - val_acc: 0.6330\n",
    "Epoch 86/100\n",
    "8000/8000 [==============================] - 4s 463us/step - loss: 0.8500 - acc: 0.7381 - val_loss: 0.9894 - val_acc: 0.6865\n",
    "Epoch 87/100\n",
    "8000/8000 [==============================] - 4s 461us/step - loss: 0.8466 - acc: 0.7403 - val_loss: 1.0206 - val_acc: 0.6585\n",
    "Epoch 88/100\n",
    "8000/8000 [==============================] - 4s 461us/step - loss: 0.8149 - acc: 0.7580 - val_loss: 0.9566 - val_acc: 0.6900\n",
    "Epoch 89/100\n",
    "8000/8000 [==============================] - 4s 458us/step - loss: 0.7948 - acc: 0.7650 - val_loss: 0.9831 - val_acc: 0.6780\n",
    "Epoch 90/100\n",
    "8000/8000 [==============================] - 4s 464us/step - loss: 0.8038 - acc: 0.7625 - val_loss: 0.9769 - val_acc: 0.6860\n",
    "Epoch 91/100\n",
    "8000/8000 [==============================] - 4s 465us/step - loss: 0.8032 - acc: 0.7611 - val_loss: 0.9174 - val_acc: 0.7120\n",
    "Epoch 92/100\n",
    "8000/8000 [==============================] - 4s 466us/step - loss: 0.7685 - acc: 0.7810 - val_loss: 0.9227 - val_acc: 0.7095\n",
    "Epoch 93/100\n",
    "8000/8000 [==============================] - 4s 465us/step - loss: 0.7576 - acc: 0.7820 - val_loss: 1.0209 - val_acc: 0.6740\n",
    "Epoch 94/100\n",
    "8000/8000 [==============================] - 4s 464us/step - loss: 0.7426 - acc: 0.7916 - val_loss: 0.9381 - val_acc: 0.6990\n",
    "Epoch 95/100\n",
    "8000/8000 [==============================] - 4s 461us/step - loss: 0.7657 - acc: 0.7803 - val_loss: 0.9433 - val_acc: 0.7040\n",
    "Epoch 96/100\n",
    "8000/8000 [==============================] - 4s 465us/step - loss: 0.7560 - acc: 0.7821 - val_loss: 0.9267 - val_acc: 0.7215\n",
    "Epoch 97/100\n",
    "8000/8000 [==============================] - 4s 463us/step - loss: 0.7044 - acc: 0.8065 - val_loss: 0.9255 - val_acc: 0.7095\n",
    "Epoch 98/100\n",
    "8000/8000 [==============================] - 4s 463us/step - loss: 0.7357 - acc: 0.7940 - val_loss: 0.9556 - val_acc: 0.7035\n",
    "Epoch 99/100\n",
    "8000/8000 [==============================] - 4s 459us/step - loss: 0.7017 - acc: 0.8055 - val_loss: 0.9035 - val_acc: 0.7330\n",
    "Epoch 100/100\n",
    "8000/8000 [==============================] - 4s 461us/step - loss: 0.7178 - acc: 0.8025 - val_loss: 0.8673 - val_acc: 0.7375\n",
    "100/100 [==============================] - 1s 7ms/step\n",
    "0.7999363780021668  0.79\n",
    "0.79\n",
    "0.9932187\n",
    "0.7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
